{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Silver', 'Gold', 'Copper', 'Palladium', 'Platinum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [10, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[[] for split in range(n_fold)] for name in names]\n",
    "\n",
    "for n in range(len(names)):\n",
    "    for split in range(n_fold):\n",
    "        models[n][split] = pickle.load(open(f'{models_dir}/trained_fullmetal_model_id_{names[n]}_fold{str(split)}.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models),len(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = 'train_metals_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 ['Price_Lag_10_Silver', 'log_close/mean_10_Silver', 'log_return_10_Silver', 'Price_Lag_20_Silver', 'log_close/mean_20_Silver', 'log_return_20_Silver', 'Price_Lag_50_Silver', 'log_close/mean_50_Silver', 'log_return_50_Silver', 'mean_close/mean_10_Silver', 'mean_log_returns_10_Silver', 'log_close/mean_10-mean_close/mean_10_Silver', 'log_return_10-mean_log_returns_10_Silver', 'mean_close/mean_20_Silver', 'mean_log_returns_20_Silver', 'log_close/mean_20-mean_close/mean_20_Silver', 'log_return_20-mean_log_returns_20_Silver', 'mean_close/mean_50_Silver', 'mean_log_returns_50_Silver', 'log_close/mean_50-mean_close/mean_50_Silver', 'log_return_50-mean_log_returns_50_Silver', 'Year', 'Month', 'Week_Number', 'Price_Lag_10_Gold', 'log_close/mean_10_Gold', 'log_return_10_Gold', 'Price_Lag_20_Gold', 'log_close/mean_20_Gold', 'log_return_20_Gold', 'Price_Lag_50_Gold', 'log_close/mean_50_Gold', 'log_return_50_Gold', 'mean_close/mean_10_Gold', 'mean_log_returns_10_Gold', 'log_close/mean_10-mean_close/mean_10_Gold', 'log_return_10-mean_log_returns_10_Gold', 'mean_close/mean_20_Gold', 'mean_log_returns_20_Gold', 'log_close/mean_20-mean_close/mean_20_Gold', 'log_return_20-mean_log_returns_20_Gold', 'mean_close/mean_50_Gold', 'mean_log_returns_50_Gold', 'log_close/mean_50-mean_close/mean_50_Gold', 'log_return_50-mean_log_returns_50_Gold', 'Price_Lag_10_Copper', 'log_close/mean_10_Copper', 'log_return_10_Copper', 'Price_Lag_20_Copper', 'log_close/mean_20_Copper', 'log_return_20_Copper', 'Price_Lag_50_Copper', 'log_close/mean_50_Copper', 'log_return_50_Copper', 'mean_close/mean_10_Copper', 'mean_log_returns_10_Copper', 'log_close/mean_10-mean_close/mean_10_Copper', 'log_return_10-mean_log_returns_10_Copper', 'mean_close/mean_20_Copper', 'mean_log_returns_20_Copper', 'log_close/mean_20-mean_close/mean_20_Copper', 'log_return_20-mean_log_returns_20_Copper', 'mean_close/mean_50_Copper', 'mean_log_returns_50_Copper', 'log_close/mean_50-mean_close/mean_50_Copper', 'log_return_50-mean_log_returns_50_Copper', 'Price_Lag_10_Palladium', 'log_close/mean_10_Palladium', 'log_return_10_Palladium', 'Price_Lag_20_Palladium', 'log_close/mean_20_Palladium', 'log_return_20_Palladium', 'Price_Lag_50_Palladium', 'log_close/mean_50_Palladium', 'log_return_50_Palladium', 'mean_close/mean_10_Palladium', 'mean_log_returns_10_Palladium', 'log_close/mean_10-mean_close/mean_10_Palladium', 'log_return_10-mean_log_returns_10_Palladium', 'mean_close/mean_20_Palladium', 'mean_log_returns_20_Palladium', 'log_close/mean_20-mean_close/mean_20_Palladium', 'log_return_20-mean_log_returns_20_Palladium', 'mean_close/mean_50_Palladium', 'mean_log_returns_50_Palladium', 'log_close/mean_50-mean_close/mean_50_Palladium', 'log_return_50-mean_log_returns_50_Palladium', 'Price_Lag_10_Platinum', 'log_close/mean_10_Platinum', 'log_return_10_Platinum', 'Price_Lag_20_Platinum', 'log_close/mean_20_Platinum', 'log_return_20_Platinum', 'Price_Lag_50_Platinum', 'log_close/mean_50_Platinum', 'log_return_50_Platinum', 'mean_close/mean_10_Platinum', 'mean_log_returns_10_Platinum', 'log_close/mean_10-mean_close/mean_10_Platinum', 'log_return_10-mean_log_returns_10_Platinum', 'mean_close/mean_20_Platinum', 'mean_log_returns_20_Platinum', 'log_close/mean_20-mean_close/mean_20_Platinum', 'log_return_20-mean_log_returns_20_Platinum', 'mean_close/mean_50_Platinum', 'mean_log_returns_50_Platinum', 'log_close/mean_50-mean_close/mean_50_Platinum', 'log_return_50-mean_log_returns_50_Platinum']\n"
     ]
    }
   ],
   "source": [
    "#df_train = reduce_mem_usage(df_train)\n",
    "train_merged = df_train.copy()\n",
    "\n",
    "not_use_features_train = ['Date']\n",
    "\n",
    "for name in names:\n",
    "    not_use_features_train.append(f'Price_{name}')\n",
    "\n",
    "features = train_merged.drop(columns=not_use_features_train).columns.to_list()\n",
    "print(len(features), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('float64'), dtype('O'), dtype('int64')], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merged.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2.16 MB\n",
      "Memory usage after optimization is: 0.62 MB\n",
      "Decreased by 71.4%\n"
     ]
    }
   ],
   "source": [
    "train_merged = reduce_mem_usage(train_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2013-04-09\n",
       "1       2013-04-10\n",
       "2       2013-04-11\n",
       "3       2013-04-12\n",
       "4       2013-04-15\n",
       "           ...    \n",
       "2474    2023-02-22\n",
       "2475    2023-02-23\n",
       "2476    2023-02-24\n",
       "2477    2023-02-27\n",
       "2478    2023-02-28\n",
       "Name: Date, Length: 2479, dtype: category\n",
       "Categories (2479, object): ['2013-04-09', '2013-04-10', '2013-04-11', '2013-04-12', ..., '2023-02-23', '2023-02-24', '2023-02-27', '2023-02-28']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merged['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lags(df):\n",
    "    lags = [10,20,50]\n",
    "    df_out = pd.DataFrame()\n",
    "\n",
    "    for col in df.columns:\n",
    "\n",
    "        for lag in lags:\n",
    "            df_out[f'{col}_Lag_{lag}'] = df[col].shift(lag)\n",
    "            df_out[f'log_close/mean_{lag}'] = np.log( np.array(df[col]) /  np.roll(np.append(np.convolve( np.array(df[col]), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1))\n",
    "            df_out[f'log_return_{lag}']     = np.log( np.array(df[col]) /  np.roll(np.array(df[col]), lag))\n",
    "\n",
    "        for lag in lags:\n",
    "            df_out[f'mean_close/mean_{lag}'] =  np.mean(df_out.iloc[:,df_out.columns.str.startswith(f'log_close/mean_{lag}')], axis=1)\n",
    "            df_out[f'mean_log_returns_{lag}'] = np.mean(df_out.iloc[:,df_out.columns.str.startswith(f'log_return_{lag}')] ,    axis=1)\n",
    "\n",
    "            df_out[f'log_close/mean_{lag}-mean_close/mean_{lag}'] = np.array( df_out[f'log_close/mean_{lag}']) - np.array( df_out[f'mean_close/mean_{lag}']  )\n",
    "            df_out[f'log_return_{lag}-mean_log_returns_{lag}']    = np.array( df_out[f'log_return_{lag}'])     - np.array( df_out[f'mean_log_returns_{lag}'] )\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def get_date_features(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Date'] = pd.to_datetime(df_copy['Date'])\n",
    "    df_copy['Year'] = df_copy['Date'].dt.year\n",
    "    df_copy['Month'] = df_copy['Date'].dt.month\n",
    "    df_copy['Week_Number'] = df_copy['Date'].dt.isocalendar().week\n",
    "    return df_copy\n",
    "\n",
    "def get_features(df, max_date):\n",
    "    #df_feat = df[['Price', 'Change_%', 'MA_for_10_days', 'MA_for_20_days', 'MA_for_50_days']].copy() # 'Open', 'High', 'Low', , \n",
    "    #df_feat = pd.concat([df_feat, get_lags(df_feat[['MA_for_10_days', 'MA_for_20_days', 'MA_for_50_days']])], axis=1)\n",
    "    #df_feat['Upper Shadow'] = upper_shadow(df_feat)\n",
    "    #df_feat['Lower Shadow'] = lower_shadow(df_feat)\n",
    "    \n",
    "    # Time window\n",
    "    df_range = df[df['Date'] >= max_date]\n",
    "\n",
    "    # Fatures to lag\n",
    "    df_feat = get_lags(df_range[['Price']])\n",
    "\n",
    "    # Date features\n",
    "    date_df = df_range[['Date']].copy()\n",
    "    date_df['Date'] = pd.to_datetime(date_df['Date'])\n",
    "    df_feat = pd.concat([df_feat, get_date_features(date_df), df_range[['Price']]], axis=1)\n",
    "    df_feat.dropna(inplace=True)\n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged['Date'] = pd.to_datetime(train_merged['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2023-02-28 00:00:00'), Timestamp('2023-03-01 00:00:00'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merged.iloc[-1]['Date'], train_merged.iloc[-1]['Date'] + pd.DateOffset(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-03-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-03-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023-03-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2023-03-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date\n",
       "0  2023-03-01\n",
       "1  2023-03-02\n",
       "2  2023-03-03\n",
       "3  2023-03-06\n",
       "4  2023-03-07\n",
       "5  2023-03-08\n",
       "6  2023-03-09\n",
       "7  2023-03-10\n",
       "8  2023-03-13\n",
       "9  2023-03-14\n",
       "10 2023-03-15\n",
       "11 2023-03-16\n",
       "12 2023-03-17\n",
       "13 2023-03-20\n",
       "14 2023-03-21\n",
       "15 2023-03-22\n",
       "16 2023-03-23\n",
       "17 2023-03-24\n",
       "18 2023-03-27\n",
       "19 2023-03-28\n",
       "20 2023-03-29\n",
       "21 2023-03-30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "\n",
    "date_range = pd.date_range(\n",
    "    train_merged.iloc[-1]['Date'] + pd.DateOffset(days=1), \n",
    "    train_merged.iloc[-1]['Date'] + pd.DateOffset(days=30), freq=us_bd)\n",
    "date_df = pd.DataFrame({'Date': date_range})\n",
    "date_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Strategy: \n",
    "    the model is trained with lags of 10, 20, and 50 days.\n",
    "    it's also trained using cv of 10, which means, it uses about 9 years of training, and 1 year of data for testing, because the date range is from 2013 to 2023.\n",
    "    we have 2 types of models, the ones trained with 5 metals standalone, and for each one we have 10 models, totalling 50 models.\n",
    "    and also 50 more models trained with the whole dataset, so it can see how the other metals behave, which gives it a more general view over the market.\n",
    "\n",
    "    1) Load the data and generate a future window of time (in us business days).\n",
    "    2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
